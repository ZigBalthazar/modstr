# Mostr

Smart Moderation for the Nostr Protocol.

---

## âœ¨ Features

* âœ… **Event Classification Engine**
  Categorize events as `SAFE`, `WARNING`, or `HARMFUL` with detailed labels like `SPAM`, `INFORMATIVE`, `OFFENSIVE`, and more.

* ğŸ¤– **AI-Powered Moderation**
  Use LLMs or custom rules to assess content safety, language, sentiment, or compliance.

* ğŸ”§ **Extensible Prompt System**
  Build structured prompts dynamically for use with OpenAI, Anthropic, or local models.

* ğŸ§  **Pluggable Analysis Modules**
  Easily integrate NLP tools, toxicity scoring, or language detection.

* ğŸŒ **Relay + Client Friendly**
  Works with relays for moderation pipelines, or clients for in-app safety.

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/ZigBalthazar/mostr.git
cd mostr
npm install
```

---

## ğŸš€ Usage

 <!-- TODO ::: -->

---

## ğŸ“– Labels & Statuses

**Statuses:**

* `SAFE`
* `WARNING`
* `HARMFUL`

**Labels:**

```json
[
  "OK", "INFORMATIVE", "FRIENDLY", "OFFENSIVE", "SPAM", "MISLEADING",
  "SENSITIVE", "PROFANITY", "HATEFUL", "VIOLENT", "SEXUAL", "SELF_HARM",
  "DANGEROUS", "NSFW", "AI_GENERATED", "UNVERIFIED", "LANGUAGE_UNKNOWN"
]
```

---

## ğŸ“š Roadmap

* [ ] Language detection integration
* [ ] Local model support (e.g., Ollama, Llama.cpp)
* [ ] NIP-75 moderation guidelines
* [ ] Real-time webhook + relay middleware
* [ ] DVM

---

## ğŸ¤ Contributing

Contributions are welcome! Please open issues or pull requests.

---

## ğŸ“„ License

MIT
